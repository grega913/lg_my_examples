{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add runtime configuration to your graph\n",
    "\n",
    "Sometimes you want to be able to configure your agent when calling it. Examples of this include configuring which LLM to use. Below we walk through an example of doing so.\n",
    "\n",
    "Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following:\n",
    "\n",
    "*   LangGraph State\n",
    "*   Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "model_OpenAI = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "def _call_model(state):\n",
    "    state[\"messages\"]\n",
    "    response = model_OpenAI.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"model\", _call_model)\n",
    "builder.add_edge(START, \"model\")\n",
    "builder.add_edge(\"model\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the graph \n",
    "Great! Now let's suppose that we want to extend this example so the user is able to choose from multiple llms. We can easily do that by passing in a config. Any configuration information needs to be passed inside configurable key as shown below. This config is meant to contain things are not part of the input (and therefore that we don't want to track as part of the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from typing import Optional\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "model_Groq = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "\n",
    "models = {\n",
    "    \"groq\": model_Groq,\n",
    "    \"openai\": model_OpenAI,\n",
    "}\n",
    "\n",
    "\n",
    "def _call_model(state: AgentState, config: RunnableConfig):\n",
    "    # Access the config through the configurable key\n",
    "    model_name = config[\"configurable\"].get(\"model\", \"groq\")\n",
    "    model = models[model_name]\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"model\", _call_model)\n",
    "builder.add_edge(START, \"model\")\n",
    "builder.add_edge(\"model\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we call it with no configuration, it will use the default as we defined it (Groq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi, what LLM model do you use', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"\\n<think>\\nThe user is asking about my model architecture. I should guide him to check Qwen's official website.\\n</think>\\n\\nFor more information about me, please visit Qwen's official website.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 19, 'total_tokens': 57, 'completion_time': 0.094624241, 'prompt_time': 0.00310284, 'queue_time': 0.09321831900000001, 'total_time': 0.097727081}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_605cd2a568', 'finish_reason': 'stop', 'logprobs': None}, id='run-b02b31cb-8152-4219-a042-bef4e8fc29ed-0', usage_metadata={'input_tokens': 19, 'output_tokens': 38, 'total_tokens': 57})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [HumanMessage(content=\"hi, what LLM model do you use\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call it with a config to get it to use a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi, what LLM model do you use', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I am based on OpenAI's GPT-3.5 architecture.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 16, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'id': 'chatcmpl-BEzCyDje6L9jH8lQonkDHoGIKzRYC', 'finish_reason': 'stop', 'logprobs': None}, id='run-8172eaf3-4bee-4d4b-9f6a-ba3d9ebda3f6-0', usage_metadata={'input_tokens': 16, 'output_tokens': 15, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"model\": \"openai\"}}\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"hi, what LLM model do you use\")]}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More configuration\n",
    "\n",
    "We can also adapt our graph to take in more configuration! Like a system message for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "# We can define a config schema to specify the configuration options for the graph\n",
    "# A config schema is useful for indicating which fields are available in the configurable dict inside the config\n",
    "class ConfigSchema(TypedDict):\n",
    "    model: Optional[str]\n",
    "    system_message: Optional[str]\n",
    "\n",
    "\n",
    "def _call_model(state: AgentState, config: RunnableConfig):\n",
    "    # Access the config through the configurable key\n",
    "    model_name = config[\"configurable\"].get(\"model\", \"groq\")\n",
    "    model = models[model_name]\n",
    "    messages = state[\"messages\"]\n",
    "    if \"system_message\" in config[\"configurable\"]:\n",
    "        messages = [\n",
    "            SystemMessage(content=config[\"configurable\"][\"system_message\"])\n",
    "        ] + messages\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph - note that we pass in the configuration schema here, but it is not necessary\n",
    "workflow = StateGraph(AgentState, ConfigSchema)\n",
    "workflow.add_node(\"model\", _call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hiHI, which model is behind you', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"\\n<think>\\nOkay, the user is asking about the model behind me. I should guide him to check Qwen's official website.\\n</think>\\n\\nFor more information about me, please visit Qwen's official website.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 18, 'total_tokens': 59, 'completion_time': 0.100081295, 'prompt_time': 0.004971981, 'queue_time': 0.19581821500000002, 'total_time': 0.105053276}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_fbb7e6cc39', 'finish_reason': 'stop', 'logprobs': None}, id='run-d934e735-d2aa-45b3-bea2-5e71db148171-0', usage_metadata={'input_tokens': 18, 'output_tokens': 41, 'total_tokens': 59})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": [HumanMessage(content=\"hiHI, which model is behind you\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
